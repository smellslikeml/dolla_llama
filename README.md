# Dolla Llama: Streaming Chat Co-Pilot for Closing the Deal ðŸ“ž

<p align="center">
  <img src="assets/dolla_llama.png" alt="Dolla Llama">
</p>

This assistant uses an advanced example of RAG (Retrieval-Augmented Generation) implementation to live assist during sales calls. It can be used as an example for your application.

## ðŸŒŸ Features:
- Custom embeddings for your text corpus using SentenceTransformers
- Indexing documents + embeddings with ElasticSearch
- Extract structured information using tools like Guidance
- Complex scoring mechanisms for improved relevance in results

## Table of Contents
1. [Getting Started](#getting-started)
2. [Creating Custom Embeddings](#creating-custom-embeddings)
3. [Indexing with ElasticSearch](#indexing-with-elasticsearch)
4. [Structured Information Extraction](#structured-information-extraction)
5. [Scoring Mechanism](#scoring-mechanism)
6. [Interface with Gradio](#interface-with-gradio)
7. [Next Steps](#next-steps)

## Getting Started
Before diving in, ensure you have the following prerequisites:

- A basic understanding of [RAG](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/) and its applications
- [docker](https://docs.docker.com/engine/install/) and [docker-compose](https://docs.docker.com/compose/install/) installed

## Creating Custom Embeddings

## Indexing with ElasticSearch

## Structured Information Extraction

## Scoring Mechanism

## Interface with Gradio

## Next Steps

* Fine-tune your own Llama model for your usecase
* Collect feedback and refine the assistant's accuracy.
* Consider using multiple indices to query/retrieve from 
* Use a container orchestrator like k8s for robust deployments
